<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1"><title>Segmentation 2 - Initial Demo</title><style> /*! normalize.css v7.0.0 | MIT License | github.com/necolas/normalize.css */html{line-height:1.15;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}article,aside,footer,header,nav,section{display:block}h1{font-size:2em;margin:.67em 0}figcaption,figure,main{display:block}figure{margin:1em 40px}hr{box-sizing:content-box;height:0;overflow:visible}pre{font-family:monospace, monospace;font-size:1em}a{background-color:transparent;-webkit-text-decoration-skip:objects}abbr[title]{border-bottom:none;text-decoration:underline;text-decoration:underline dotted}b,strong{font-weight:inherit}b,strong{font-weight:bolder}code,kbd,samp{font-family:monospace, monospace;font-size:1em}dfn{font-style:italic}mark{background-color:#ff0;color:#000}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}audio,video{display:inline-block}audio:not([controls]){display:none;height:0}img{border-style:none}svg:not(:root){overflow:hidden}button,input,optgroup,select,textarea{font-family:sans-serif;font-size:100%;line-height:1.15;margin:0}button,input{overflow:visible}button,select{text-transform:none}button,html [type='button'],[type='reset'],[type='submit']{-webkit-appearance:button}button::-moz-focus-inner,[type='button']::-moz-focus-inner,[type='reset']::-moz-focus-inner,[type='submit']::-moz-focus-inner{border-style:none;padding:0}button:-moz-focusring,[type='button']:-moz-focusring,[type='reset']:-moz-focusring,[type='submit']:-moz-focusring{outline:1px dotted ButtonText}fieldset{padding:.35em .75em .625em}legend{box-sizing:border-box;color:inherit;display:table;max-width:100%;padding:0;white-space:normal}progress{display:inline-block;vertical-align:baseline}textarea{overflow:auto}[type='checkbox'],[type='radio']{box-sizing:border-box;padding:0}[type='number']::-webkit-inner-spin-button,[type='number']::-webkit-outer-spin-button{height:auto}[type='search']{-webkit-appearance:textfield;outline-offset:-2px}[type='search']::-webkit-search-cancel-button,[type='search']::-webkit-search-decoration{-webkit-appearance:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}details,menu{display:block}summary{display:list-item}canvas{display:inline-block}template{display:none}[hidden]{display:none}.highlight{background:#282a36;color:#f8f8f2}.highlight .hll,.highlight .s,.highlight .sa,.highlight .sb,.highlight .sc,.highlight .dl,.highlight .sd,.highlight .s2,.highlight .se,.highlight .sh,.highlight .si,.highlight .sx,.highlight .sr,.highlight .s1,.highlight .ss{color:#f1fa8c}.highlight .go{color:#44475a}.highlight .err,.highlight .g,.highlight .l,.highlight .n,.highlight .x,.highlight .p,.highlight .ge,.highlight .gr,.highlight .gh,.highlight .gi,.highlight .gp,.highlight .gs,.highlight .gu,.highlight .gt,.highlight .ld,.highlight .no,.highlight .nd,.highlight .ni,.highlight .ne,.highlight .nn,.highlight .nx,.highlight .py,.highlight .w,.highlight .bp{color:#f8f8f2}.highlight .gh,.highlight .gi,.highlight .gu{font-weight:bold}.highlight .ge{text-decoration:underline}.highlight .bp{font-style:italic}.highlight .c,.highlight .ch,.highlight .cm,.highlight .cpf,.highlight .c1,.highlight .cs{color:#6272a4}.highlight .kd,.highlight .kt,.highlight .nb,.highlight .nl,.highlight .nv,.highlight .vc,.highlight .vg,.highlight .vi,.highlight .vm{color:#8be9fd}.highlight .kd,.highlight .nb,.highlight .nl,.highlight .nv,.highlight .vc,.highlight .vg,.highlight .vi,.highlight .vm{font-style:italic}.highlight .na,.highlight .nc,.highlight .nf,.highlight .fm{color:#50fa7b}.highlight .k,.highlight .o,.highlight .cp,.highlight .kc,.highlight .kn,.highlight .kp,.highlight .kr,.highlight .nt,.highlight .ow{color:#ff79c6}.highlight .m,.highlight .mb,.highlight .mf,.highlight .mh,.highlight .mi,.highlight .mo,.highlight .il{color:#bd93f9}.highlight .gd{color:#f55}*{margin:0;padding:0}*,*:before,*:after{box-sizing:inherit}html{box-sizing:border-box;font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{color:#282a36;font-family:-apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol';font-size:16px;line-height:1.5;word-wrap:break-word}a{color:#0366d6;font-weight:700;text-decoration:none}a:hover,a:active{text-decoration:underline}img{border-style:none;box-sizing:content-box;max-width:100%}h1,h2,h3,h4,h5,h6{font-weight:700;margin-top:40px;margin-bottom:10px;line-height:1.1}h1{font-size:2.25em}h2{font-size:1.5em}h3{font-size:1.25em}h4{font-size:1em}h5{font-size:.875em}h6{font-size:.75em}p,ul,ol{margin-bottom:20px}ul,ol{padding:0 0 0 40px;margin:0 0 20px 0}ol ol,ul ol{list-style-type:lower-roman}ul ul ol,ul ol ol,ol ul ol,ol ol ol{list-style-type:lower-alpha}li{margin-bottom:10px}input,select,textarea,button{font-family:inherit;font-size:inherit;line-height:inherit}pre{margin:40px -36vw;padding:40px 36vw;overflow:auto;font-size:1em;line-height:1.5;white-space:pre;word-wrap:normal;font-family:'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace}pre code{background-color:transparent;border-radius:0;font-size:100%;padding:0}code,tt{padding:5px;margin:0;font-size:85%;background-color:rgba(27,31,35,0.05);border-radius:3px;font-family:'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace}small{font-size:90%}strong{font-weight:700}.anchorjs-link,.anchorjs-link:hover{text-decoration:none !important}.visible-area{overflow:hidden}.container{margin:80px auto 40px;max-width:750px;padding:0 20px;width:100%}.introduction{margin:0;line-height:1.5}.list__item{margin-top:20px}.post__date{font-weight:700;margin:0 0 10px 0;text-transform:uppercase}.post__title{font-size:2.25em;margin:0 0 10px 0}@media only screen and (min-width: 768px){.post__title{font-size:3.75em}}.post__author{margin:0 0 40px 0}footer{color:#6a737d;display:flex;font-size:80%;justify-content:space-between;margin-top:40px}footer p{margin-bottom:0}.blue{color:#0366d6}.monospace{font-family:'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace}</style><body><div class="visible-area"><div class="container"><header><p class="post__date">20 September 2017<h1 class="post__title"><a name="top"></a>Segmentation 2 - Initial Demo</h1><p class="post__author"><em>by</em> <a href="https://levinejh.github.io/">Joe Levine</a></header><p>Here’s where we dive into the nitty gritty technical details. This post describes training a full working network. The complete description is somewhat involved, so please have some patience. In contrast, the training code itself is quite short! Once you understand the basic idea it becomes straightforward to follow. I promise. :)<h3 id="before-you-dive-in-heres-where-well-end-up">Before you dive in, here’s where we’ll end up.</h3><p>This post is long. To first orient ourselves, let me summarize where we’ll end up. We’ll create and train a deep convolutional neural network to segment bacterial cells in phase contrast microscopy images, coding in Tensorflow. This training code will take less than 200 lines.<p>To be very concrete, we’re first going to train a network to segment the training image below. The trained network will produce a mask like the one shown. Once trained, it will then segment similiar test images with high accuracy. We’ll show the results for both.<p><img src="assets/2017-09-20-initial-montage.png" alt="alt text" /><h3 id="development-and-computation">Development and Computation</h3><p>Let’s pause first and consider two people.<p>The first: a fan of modern young-adult-dystopian fantasy, planning a movie night.<p>The second: a student of modern machine learning, looking for a development environment.<p>What do these two people have in common?<div style="text-align:center"> <img src="assets/2017-09-20-maxresdefault.jpg" /><figcaption>With today's Machine Learning development packages, the odds are ever in your favor.</figcaption></div><p><br /> <br /><p>Both have several options.<p>Several high quality open source machine learning environments exist. Examples include <a href="http://pytorch.org/">PyTorch</a>, <a href="http://caffe.berkeleyvision.org/">Caffe</a>, <a href="https://keras.io/">Keras</a>, and <a href="https://www.tensorflow.org/">Tensorflow</a>. I ended up choosing Tensorflow for this demonstration. It’s a Google-backed open source environment with a robust online comunity.  <p>Very roughly, Tensorflow treats neural networks as a graph. A network’s input, hidden, and output layers are graph vertices. A network’s weights are graph edges. Training adjusts the values in these weights, and Tensorflow provides powerful automatic optimization tools for adjusting these weights which simplify numerical coding. To compute values, graph inputs are define and the results flow through the graph, where they can be used to optimize the network’s weights for training. The end result lets one focus on designing a specific neural network architecture for one’s specific application, rather than worrying about re-implementing common numerical computations correctly. All of this is done in the friendly and fungible Python language. Here’s a great animation from Google illustrating how Tensorflow works - we’ll be explaining pieces of this for our network below.<div style="text-align:center"> <img src="assets/2017-09-20-tensors_flowing.gif" /><figcaption>Value propagation and training in Tensorflow</figcaption></div><p><br /> <br /> Second, what computing environment should one use? I’ll spare you the another dystopian fantasy image this time, but rest assured the odds here are also ever in your favor. I did all bacterial segmentation work using readily available resources - a personal laptop and Amazon Web Services for cloud computing. I performed most of the heavy lifting in the cloud - specifically a g2.xlarge GPU-containing Amazon Elastic Cloud Compute instance. The EC2 instance is noticeably faster than a laptop, and conveniently can be configured pre-installed with the necessary deep-learning software packages (Tensorflow, NumSciPy, etc - I used Amazon’s Deep Learning AMI AmazonLinux - 2.0 (aml-dfb13ebf)) - this saves a huge amount of headache.<h3 id="training-data">Training Data</h3><p>Here’s a remarkable thing about this pixel-wise segmentation task - you don’t need a lot of images to get a lot of training data!<p>In fact, I trained this network with one image.<p>How could this be? The intuitive reason is that a single image contains roughly 1,000 x 1,000 = 1,000,000 pixels. Each of these pixels provides a training example for pixel classification, therefore we get roughly 1,000,000 training examples per image. Pixel-patch overlap admittedly makes these images somewhat correlated, but despite this the training empirically works well.<p>To turn this training image into a training data set, I manually segmented the single raw microscopy image shown below. I created three separate masks - one for cells, one for edges, and one for everything else (‘other’). For each pixel location in the original image, only one of the three masks has value 1 - the other two are zero. This provides the training class labels for each pixel. There are several mechanical ways to make these three images - I used the open source microscopy image analysis program <a href="https://imagej.nih.gov/ij/">ImageJ</a>, which has simple Region of Interest (ROI) tools that can do this.<div style="text-align:center"> <img src="assets/2017-09-20-training-and-masks.png" /><figcaption>Training Data: Original Image (left) and cell, edge, and other masks. Bottom row shows a zoomed in patch.</figcaption></div><p><br /> <br /><p>I want to emphasize that creating this labeled training data is tedious. No surprise here - labeled training data is almost always expensive. I recommend you beg for, borrow, or buy someone else’s time to do this.<p>With all those preliminaries aside, let’s get down to some initial code.<h3 id="code">Code</h3><p>Implementing this basic segmentation task in Tensorflow turns out to be remarkably simple. The majority of the code here is actually supporting material - training data loading and sampling, and network and error saving during the training process. The actual network definition and computation pieces themselves are brief.<h4 id="import-statements">Import Statements</h4><p>We first import the necessary packages - Numpy to handle arrays, Random to enable random sampling, and Tensorflow itself.<div class="highlighter-rouge"><pre class="highlight"><code>import numpy as np
import random
import tensorflow as tf
</code></pre></div><h4 id="hyperparameters">Hyperparameters</h4><p>We next define several hyper-parameters used during training. These parameters control details of the learning process such as the number of gradient descent iterations to perform (‘epochs’), the number of examples to present during each gradient descent step (‘batch size’), the size of the pixel-patch surrounding each pixel, and the optimizer’s learning rate. We also define here numerical parameters for saving frequency.<div class="highlighter-rouge"><pre class="highlight"><code>nepochs = 300000
batchSize = 256
patchSize = 15
learning_rate = 0.0001
interval_display = 50
interval_save = 10000
</code></pre></div><h4 id="training-data-loading">Training Data Loading</h4><p>Here we load the training image and class label masks. To make random sampling easier, we then extract the indices of pixels for each class from the respective masks. Finally, we define the path to save the weights and training curves from this particular experiment.<div class="highlighter-rouge"><pre class="highlight"><code>save_path = '/home/ec2-user/Segmentation/Weights/segmentation_experiment_01/'
mask_path = '/home/ec2-user/Segmentation/Masks/'
image_path = '/home/ec2-user/Segmentation/Images/'

im = plt.imread(image_path + 'phase_27.tif')
mask_cells = np.load(mask_path + 'image_027_mask_cells.npy')
mask_edges = np.load(mask_path + 'image_027_mask_edges.npy')
mask_other = np.load(mask_path + 'image_027_mask_other.npy')

inds_cells = np.ravel_multi_index(np.where(mask_cells &gt; 0), mask_cells.shape)
inds_edges = np.ravel_multi_index(np.where(mask_edges &gt; 0), mask_edges.shape)
inds_other = np.ravel_multi_index(np.where(mask_other &gt; 0), mask_other.shape)

</code></pre></div><h4 id="helper-functions">Helper Functions</h4><p>Next we define a few helper fuctions to make the code more readable. The first two simplify expression of the standard convolutional neural network Convolution and MaxPool operations. I copied these directly from the Tensorflow tutorial.<div class="highlighter-rouge"><pre class="highlight"><code>def conv2d(x, W):
    return tf.nn.conv2d(x, W, strides = [1,1,1,1], padding = 'VALID')

def maxpool2x2(x):
    return tf.nn.max_pool(x, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'VALID')
</code></pre></div><p>The next two simplify declaration of weight and bias variables. Again - these are copied from the Tensorflow tutorial.<div class="highlighter-rouge"><pre class="highlight"><code>def weight_variable(shape):
    initial = tf.truncated_normal(shape, stddev = 0.1)
    return tf.Variable(initial)

def bias_variable(shape):
    initial = tf.constant(0.1, shape = shape)
    return tf.Variable(initial)
</code></pre></div><p>The final two functions are helper functions. The first implements random sampling for batches. One key feature to note here is that I draw uniformly from the three classes. This ensures that the network is exposed to a similar number of examples of each class. The second saves weights as .npy files.<div class="highlighter-rouge"><pre class="highlight"><code>def getBatch(batchSize, patchSize, rawImage, ind_cells, ind_edges, ind_other):
    # This function randomly samples pixels from the image, sampling uniformly
    # across the three classes 'cell', 'edge', 'other', indexing from the ind arrays.
    # It draws 'batchSize' samples of size 2*patchSize+1 x 2*patchSize+1
    #
    # Function returns (imageBatch, labelBatch)
    # imageBatch is size (batchSize, 2*patchSize+1, 2*patchSize+1,1) - the samples
    # labelBatch is size (batchSize, 3) - one-hot class labels.
    #
    # Code omitted for brevity

def save_weights(file_header, weight_dict):
  # This function saves the weights in weight_dict using header file_header.
  # Code omitted for brevity.

</code></pre></div><h4 id="network-structure">Network Structure</h4><p>Our network follows the standard architecture of basic convolutional networks - alternating layers of convolutions, RelU nonlinearities, and Max-Pool operations. The specific layers are: <center><table style="width:80%"><tr><td><b>Layer</b><td><b>Description</b><td><b>Output Size</b><tr><td>Input<td>Pixel-patch<td>31x31x1 pixels<tr><td>Convolution + RelU<td>Field: 4x4, Stride: 1, Depth: 20<td>28x28x20<tr><td>Max Pool<td>Field: 2x2, Stride: 2<td>14x14x20<tr><td>Convolution + RelU<td>Field: 3x3, Stride: 1, Depth: 40<td>12x12x40<tr><td>Convolution + RelU<td>Field: 3x3, Stride: 1, Depth: 80<td>10x10x80<tr><td>Max Pool<td>Field: 2x2, Stride: 2<td>5x5x80<tr><td>Convolution + RelU<td>Field: 3x3, Stride: 1, Depth: 120<td>3x3x120<tr><td>Convolution + RelU<td>Field: 3x3, Stride: 1, Depth: 240<td>1x1x240<tr><td>Fully Connected + RelU<td>240x1000 Matrix<td>1x1000<tr><td>Class Probabilities<td>1000x3 Matrix<td>1x3</table></center><p><br /> <br /><p>I’ll discuss this architecture’s rationale in the next post. To implement this architecture, we first define the appropriate Tensorflow placeholders and variables:<div class="highlighter-rouge"><pre class="highlight"><code>x = tf.placeholder(tf.float32, [batchSize, 2*patchSize+1, 2*patchSize+1, 1])
y_ = tf.placeholder(tf.float32, [batchSize, 3])
W1 = weight_variable([4,4,1,20])
b1 = bias_variable([20])
W2 = weight_variable([3,3,20,40])
b2 = bias_variable([40])
W3 = weight_variable([3,3,40,80])
b3 = bias_variable([80])
W4 = weight_variable([3,3,80,120])
b4 = bias_variable([120])
W5 = weight_variable([3,3,120,240])
b5 = weight_variable([240])
Wfc = weight_variable([240,1000])
bfc = bias_variable([1000])
Wout = weight_variable([1000,3])
bout = bias_variable([3])
</code></pre></div><p>Next we define the operations connecting them:<div class="highlighter-rouge"><pre class="highlight"><code>h1_conv = tf.nn.relu( conv2d(x, W1) + b1 )
h1_pool = maxpool2x2(h1_conv)
h2_conv = tf.nn.relu(conv2d(h1_pool, W2) + b2)
h3_conv = tf.nn.relu(conv2d(h2_conv, W3) + b3)
h3_pool = maxpool2x2(h3_conv)
h4_conv = tf.nn.relu(conv2d(h3_pool, W4) + b4)
h5_conv = tf.nn.relu(conv2d(h4_conv, W5) + b5)
h5_flat = tf.reshape(h5_conv, [-1, 240])
h_fc = tf.nn.relu( tf.matmul(h5_flat, Wfc) + bfc )
y_conv = tf.matmul(h_fc, Wout) + bout
</code></pre></div><h4 id="tensorflow-graph-initialization">Tensorflow graph initialization</h4><p>The next code sections initialize the remaining necessary portions of the Tensorflow graph - the objective function to be optimized and the specific optimizer used. Since we’re predicting a class probability for each pixel, I will use the standard cross-entropy cost function to measure the disparity between the network’s prediction of a pixel’s class and the actual class label defined in training.<p>Next, I chose the ADAM method to perform gradient descent on this cost function.<p>Next, although the cross entropy has desirable mathematical properties for optimization, it’s numerically hard to interpret. I therefore compute the fraction of examples classified correctly in each batch, to let us intuitively track training performance.<p>Lastly, of course, we initialize all global variables with Tensorflow.<div class="highlighter-rouge"><pre class="highlight"><code>loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))
train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)
correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
init = tf.global_variables_initializer()
</code></pre></div><h4 id="training-loop">Training loop</h4><p>Now it’s time to get to the meat - the training! This code is extremely simple. It starts a Tensorflow session, and during this session iterates for 300,000 epochs. During each epoch, the code performs a gradient descent calculation. To do this, it randomly samples a set (‘batch’) of pixel-patches from the training image, and evaluates <code class="highlighter-rouge">train_step</code> using this batch as the input to the placeholder <code class="highlighter-rouge">x</code>. <code class="highlighter-rouge">train_step</code> evaluates the network with the current weight matrices, calculates the cross-entropy cost function between the network’s classification of the batches and their labels, and performs the gradient descent step with the Adam optimizer.<div class="highlighter-rouge"><pre class="highlight"><code>with tf.Session() as sess:
    sess.run(init)

    for i in range(nepochs):
        batch = getBatch(batchSize, patchSize, im, inds_cells, inds_edges, inds_other)
        sess.run(train_step, feed_dict={x: batch[0], y_: batch[1]})

        # Also some additional code (not shown) to save weight matrices, training loss, and accuracy
</code></pre></div><h3 id="training-progression">Training Progression</h3><p>It took me roughly 6 hours to train a network for 300,000 epochs on an Amazon <code class="highlighter-rouge">g2.xlarge</code> EC2 GPU-enabled instance. Training this network typically proceeds very smoothly. One way to quantify this is by plotting how the training accuracy improves during the training process. Below, we plot training mistake percentage - the percent of examples in each batch the network makes a mistake on at each training epoch. We plot this curve for three different training runs.<div style="text-align:center"> <img src="assets/2017-09-20-learning-curves.png" /><figcaption>Learning curves for three training runs of our network. Pixel classification error drops stochastically, but reproducibly, during all three runs to the minimum resolution detectable by our batch size.</figcaption></div><p><br /> <br /><p>Several features pop out from these plots:<ol><li>The curves behave very similarly to one another - <em>training behaves reproducibly</em>.<li>Misclassification percentage decreases in a noisy fashion - we’re performing <em>stochastic gradient descent</em>.<li>The network improves until hitting the resolution limit of our batch size, <code class="highlighter-rouge">1/256 ~ 0.00391</code></ol><h3 id="performance">Performance</h3><p>Let’s visualize how well our network performs at segmenting actual images.<p>Recall that the network actually works by classifying each pixel in an image into one of three classes - {cell, edge, other}. More specifically, each pixel is assigned a 3-dimensional class probability vector{p(cell), p(edge), p(other)}, with p(cell) + p(edge) + p(other) = 1. Note that I’m not showing code - yet - for the actual classification computation. We’ll do that in another post. :)<p>Here’s a plot of a segmentation of the original training image, along with the corresponding computed class probabilities (cell, edge, other).<div style="text-align:center"> <img src="assets/2017-09-20-montage-train-class-probabilities.png" /><figcaption>Training image, and class probabilities for cells, edges, and other pixels predicted by the CNN after training on this image. Bottom row shows a zoom in on a representative section.</figcaption></div><p><br /> <br /><p>Now let’s segment an image that the network <em>wasn’t</em> trained on, in other words a test image.<div style="text-align:center"> <img src="assets/2017-09-20-montage-test-class-probabilities.png" /><figcaption>Testing image, and class probabilities for cells, edges, and other pixels predicted by a trained CNN. Bottom row shows a zoom in on a representative section.</figcaption></div><p><br /> <br /><p>To turn these into a final prediction, we simply threshold the cell class probability map (all pixels with cell probability greater than 0.8) to convert it into a mask.<div style="text-align:center"> <img src="assets/2017-09-20-segmented-mask-test.png" /><figcaption>Test image, segmentation mask, and image-mask overlay. Seg, and class probabilities for cells, edges, and other pixels predicted by a trained CNN. Bottom row shows a zoom in on a representative section.</figcaption></div><p><br /> <br /><p>As you can see, the segmentation is still good, but not perfect. This isn’t surprising for such a simple network. Here are a few of the issues we see:<ol><li>The class probability predictions look “rougher”. For example, the cell edges are jagged.<li>Cells contain little ‘holes’ in them.</ol><p>The core cause of these effects is simple - each pixel is classified independently of others. Sure, the patches used to compute class probabilities for two adjacent pixels may overlap, but there is no sense of connected components or pixels.<p>How can we address this problem simply and effectively? <em>Averaging</em> provides a solution. In particular, I’ll use two types of averaging - averaging over networks, and averaging over space. What do I mean by these?<p><em>Averaging over networks</em> means that we average the class probabilities produced by several networks into a single, average class probability. Why might this work? Intuitively, recall that we train the network using <em>stochastic</em> gradient descent. The training process is random because we choose pixel batches for each gradient descent step randomly. This means that if we repeat the network training process several times, the resulting weight matrices will be different each time. This suggests that averaging the classification results of matrices from multiple training runs will average out some of the more idiosyncratic classifications. I chose to average over 3 independently trained networks.<div style="text-align:center"> <img src="assets/2017-09-20-class-probabilities-cells-3-networks.png" /><figcaption>Class probabilities on a single image patch computed by three different trained networks, and their average. (Right) Probability colormap.</figcaption></div><p><br /> <br /> <em>Averaging over space</em> means that we average the class probabilities over adjacent pixels. Here, we take into account our prior knowledge that cell pixels should cluster together. I chose to run a median filter over adjacent pixels.<p>It turns out that combining these two techniques, averaging over realizations and averaging over space, works well to clean up the image.<div style="text-align:center"> <img src="assets/2017-09-20-median-filter-final.png" /><figcaption>Class probabilities on a single image patch computed averaging by three different trained networks (left), median filtering the averaged probabilities (center), and thresholded (p&gt;0.8) into a segmentation mask.</figcaption></div><p><br /> <br /><p>And now that we have these techniques, we can finally create our final workable segmentation mask.<div style="text-align:center"> <img src="assets/2017-09-20-segmented-mask-test-network-avg-median-filt.png" /><figcaption>Raw image (left), final segmentation mask from averaged networks with median filtering (center), and image-mask overlay (right).</figcaption></div><p><br /> <br /><p>Let’s roughly quantify how well these networks perform against each other in training and testing. A first cut at this checks whether the cells we get from different segmentations are the same size.<p>To check this, I segmented three images using our trained networks and the post-processing described above. All three images were taken under identical illumination conditions. The first was the original training image, and the next two were test images. I then extracted the size, in pixels, of each object in each of the masks using <code class="highlighter-rouge">scipy.ndimage.measurements.label</code>. For comparison, I also extracted the size of objects in the original hand-labeled mask. I then plotted the size distributions, shown below. Note that they look very similiar, which is encouraging! Two-sample Kolmogorov-Smirnov tests on all pairs of these empirical distributions (<code class="highlighter-rouge">scipy.stats.ks_2samp</code>) give p-Values all larger than 0.24, which supports the similarity of the distributions.<div style="text-align:center"> <img src="assets/2017-09-20-cell-size-distributions.png" /><figcaption>Distribution of cell sizes (pixels) for CNN-based segmentation masks from several images.</figcaption></div><p><br /> <br /> (A sharp eye will actually spot an interesting idiosyncrasy in this graph - I’ll leave it, and its explanation, as an exercise for the reader. :)<h3 id="conclusion">Conclusion</h3><p>Let’s summarize what we’ve done here. We’ve created and trained a simple convolutional neural network that segments bacterial cells in micrscopy images. The network trains consistently and segments test images with high accuracy, especially after incorporating some simple post-processing. The code to implement this in Tensorflow is short and transparent, weighing in at under 200 lines. You can find materials for this example (code, trained networks, and training and test images), in my repo.<div style="text-align:center"> <img src="assets/2017-09-20-sea-otter-says-good-job.jpg" /><figcaption>Sea otter says "Good job!!"</figcaption></div><p><br /> <br /><p>In the next two posts, we’ll dig a bit deeper into our choices of neural network <a href="https://levinejh.github.io/segmentation-architectures">architecture</a>. We’ll also describe how segment images with these trained networks <a href="https://levinejh.github.io/segmentation-fully-connected">quickly and efficiently</a>.<footer><p>© 2017 Feather<p><a href="#top">[Back to Top]</a></footer></div>
