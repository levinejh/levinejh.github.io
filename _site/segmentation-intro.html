<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1"><title>Segmentation 1 - Intro</title><style> /*! normalize.css v7.0.0 | MIT License | github.com/necolas/normalize.css */html{line-height:1.15;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}article,aside,footer,header,nav,section{display:block}h1{font-size:2em;margin:.67em 0}figcaption,figure,main{display:block}figure{margin:1em 40px}hr{box-sizing:content-box;height:0;overflow:visible}pre{font-family:monospace, monospace;font-size:1em}a{background-color:transparent;-webkit-text-decoration-skip:objects}abbr[title]{border-bottom:none;text-decoration:underline;text-decoration:underline dotted}b,strong{font-weight:inherit}b,strong{font-weight:bolder}code,kbd,samp{font-family:monospace, monospace;font-size:1em}dfn{font-style:italic}mark{background-color:#ff0;color:#000}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}audio,video{display:inline-block}audio:not([controls]){display:none;height:0}img{border-style:none}svg:not(:root){overflow:hidden}button,input,optgroup,select,textarea{font-family:sans-serif;font-size:100%;line-height:1.15;margin:0}button,input{overflow:visible}button,select{text-transform:none}button,html [type='button'],[type='reset'],[type='submit']{-webkit-appearance:button}button::-moz-focus-inner,[type='button']::-moz-focus-inner,[type='reset']::-moz-focus-inner,[type='submit']::-moz-focus-inner{border-style:none;padding:0}button:-moz-focusring,[type='button']:-moz-focusring,[type='reset']:-moz-focusring,[type='submit']:-moz-focusring{outline:1px dotted ButtonText}fieldset{padding:.35em .75em .625em}legend{box-sizing:border-box;color:inherit;display:table;max-width:100%;padding:0;white-space:normal}progress{display:inline-block;vertical-align:baseline}textarea{overflow:auto}[type='checkbox'],[type='radio']{box-sizing:border-box;padding:0}[type='number']::-webkit-inner-spin-button,[type='number']::-webkit-outer-spin-button{height:auto}[type='search']{-webkit-appearance:textfield;outline-offset:-2px}[type='search']::-webkit-search-cancel-button,[type='search']::-webkit-search-decoration{-webkit-appearance:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}details,menu{display:block}summary{display:list-item}canvas{display:inline-block}template{display:none}[hidden]{display:none}.highlight{background:#282a36;color:#f8f8f2}.highlight .hll,.highlight .s,.highlight .sa,.highlight .sb,.highlight .sc,.highlight .dl,.highlight .sd,.highlight .s2,.highlight .se,.highlight .sh,.highlight .si,.highlight .sx,.highlight .sr,.highlight .s1,.highlight .ss{color:#f1fa8c}.highlight .go{color:#44475a}.highlight .err,.highlight .g,.highlight .l,.highlight .n,.highlight .x,.highlight .p,.highlight .ge,.highlight .gr,.highlight .gh,.highlight .gi,.highlight .gp,.highlight .gs,.highlight .gu,.highlight .gt,.highlight .ld,.highlight .no,.highlight .nd,.highlight .ni,.highlight .ne,.highlight .nn,.highlight .nx,.highlight .py,.highlight .w,.highlight .bp{color:#f8f8f2}.highlight .gh,.highlight .gi,.highlight .gu{font-weight:bold}.highlight .ge{text-decoration:underline}.highlight .bp{font-style:italic}.highlight .c,.highlight .ch,.highlight .cm,.highlight .cpf,.highlight .c1,.highlight .cs{color:#6272a4}.highlight .kd,.highlight .kt,.highlight .nb,.highlight .nl,.highlight .nv,.highlight .vc,.highlight .vg,.highlight .vi,.highlight .vm{color:#8be9fd}.highlight .kd,.highlight .nb,.highlight .nl,.highlight .nv,.highlight .vc,.highlight .vg,.highlight .vi,.highlight .vm{font-style:italic}.highlight .na,.highlight .nc,.highlight .nf,.highlight .fm{color:#50fa7b}.highlight .k,.highlight .o,.highlight .cp,.highlight .kc,.highlight .kn,.highlight .kp,.highlight .kr,.highlight .nt,.highlight .ow{color:#ff79c6}.highlight .m,.highlight .mb,.highlight .mf,.highlight .mh,.highlight .mi,.highlight .mo,.highlight .il{color:#bd93f9}.highlight .gd{color:#f55}*{margin:0;padding:0}*,*:before,*:after{box-sizing:inherit}html{box-sizing:border-box;font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{color:#282a36;font-family:-apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol';font-size:16px;line-height:1.5;word-wrap:break-word}a{color:#0366d6;font-weight:700;text-decoration:none}a:hover,a:active{text-decoration:underline}img{border-style:none;box-sizing:content-box;max-width:100%}h1,h2,h3,h4,h5,h6{font-weight:700;margin-top:40px;margin-bottom:10px;line-height:1.1}h1{font-size:2.25em}h2{font-size:1.5em}h3{font-size:1.25em}h4{font-size:1em}h5{font-size:.875em}h6{font-size:.75em}p,ul,ol{margin-bottom:20px}ul,ol{padding:0 0 0 40px;margin:0 0 20px 0}ol ol,ul ol{list-style-type:lower-roman}ul ul ol,ul ol ol,ol ul ol,ol ol ol{list-style-type:lower-alpha}li{margin-bottom:10px}input,select,textarea,button{font-family:inherit;font-size:inherit;line-height:inherit}pre{margin:40px -36vw;padding:40px 36vw;overflow:auto;font-size:1em;line-height:1.5;white-space:pre;word-wrap:normal;font-family:'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace}pre code{background-color:transparent;border-radius:0;font-size:100%;padding:0}code,tt{padding:5px;margin:0;font-size:85%;background-color:rgba(27,31,35,0.05);border-radius:3px;font-family:'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace}small{font-size:90%}strong{font-weight:700}.anchorjs-link,.anchorjs-link:hover{text-decoration:none !important}.visible-area{overflow:hidden}.container{margin:80px auto 40px;max-width:750px;padding:0 20px;width:100%}.introduction{margin:0;line-height:1.5}.list__item{margin-top:20px}.post__date{font-weight:700;margin:0 0 10px 0;text-transform:uppercase}.post__title{font-size:2.25em;margin:0 0 10px 0}@media only screen and (min-width: 768px){.post__title{font-size:3.75em}}.post__author{margin:0 0 40px 0}footer{color:#6a737d;display:flex;font-size:80%;justify-content:space-between;margin-top:40px}footer p{margin-bottom:0}.blue{color:#0366d6}.monospace{font-family:'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace}</style><body><div class="visible-area"><div class="container"><header><p class="post__date">18 September 2017<h1 class="post__title"><a name="top"></a>Segmentation 1 - Intro</h1><p class="post__author"><em>by</em> <a href="https://levinejh.github.io/">Joe Levine</a></header><p>Extracting value from data takes work. People like saying “<a href="https://www.economist.com/news/leaders/21721656-data-economy-demands-new-approach-antitrust-rules-worlds-most-valuable-resource">data is the new oil</a>” to emphasize data’s inherent value, and perhaps just as importantly the work required to transform raw data into something valuable. It turns out extracting value can take lots of skilled work.<p>Let’s walk through such a process using an example near and dear to my heart - finding individual cells in microscopy images. Here’s an example!<p><img src="assets/2017-09-18-segmentation-concept.png" alt="alt text" /><p>You may be asking yourself, “Why on earth did he pick this example?”<p>Here’s why - I spent years of my systems biology PhD analyzing tens of thousands of these images, to understand the biology of how genes control cells. Our lab automated this process using good software. Unfortunately, each new cell shape required us to design and write custom software. Some cells, like bacteria, were rod shaped. Some, like baker’s yeast, were spherical. Some, like stem cells, were weirdly shaped blobs that looked like they came from a discarded Salvador Dali painting. Some shapes were so complicated we couldn’t figure out the right rules to find cells.<p>A general purpose method to find arbitrary cell types would have been extremely valuable. The method described in these posts provides a starting point to such a general tool.<p>Why not pick cells out by by hand? Well, that’s one of the most soul-sucking things you can do in front of a computer. Consider this typical phase contrast microscopy image of a <em>Bacillus subtilis</em> bacteria microcolony taken at 100x magnification.<p><img src="assets/2017-09-18-segmentation-big.png" alt="alt text" /><p>If you want to label an image like this by hand, good luck - enjoy the next several hours of your life drawing little red blobs. I did this once, tracing them one at a time using the specialized microscopy software <a href="https://imagej.nih.gov/ij/">ImageJ</a>. This took several (and here several does not mean 3) hours. :(<p>There’s a name for this process - “<a href="https://en.wikipedia.org/wiki/Image_segmentation">segmentation</a>.” At the most basic level, segmentation involves classifying each pixel in an image as a certain type. Here, types are either a cell (in red, above), or something else.<p>Let’s teach a computer how to segment these images automatically, so we don’t have to!<p><img src="assets/2017-09-18-machine-learning-algorithms.jpg" alt="alt text" /><p>A first thought might be to classify each pixel by its intensity. Maybe cell pixels are darker and background pixels are lighter. What could be simpler, easier, or faster? Unfortunately, this approach hits a brick wall fast, which you can see by looking at the distributions of pixel intensities in the image. There’s lots of overlap, and it’s impossible to accurately classify pixels by intensity alone.<div style="text-align:center"> <img src="assets/2017-09-18-pixel-intensity-distributions.png" /><figcaption>Pixel intensity distributions for cell pixels and non-cell pixels show substantial overlap.</figcaption></div><p><br /> <br /><p>Since we can’t classify pixels by intensity alone, we need to use extra information. Looking at a pixel’s neighbors gives us this extra information. For example, cell pixels tend to be surrounded first by other dark cell-like neighbors and then brighter pixels corresponding to spaces between the cells. Background pixels tend to be surrounded by other pixels of fairly uniform intensity. In the following, therefore, we’ll use a patch centered on each pixel of interest to classify it.<div style="text-align:center"> <img src="assets/2017-09-18-two-patches.png" /><figcaption>31x31 pixel-patches centered on a 'cell' pixel (left) and a 'non-cell' pixel (right). Although the intensities of these two pixels differ by less than 1%, their surroundings look very different.</figcaption></div><p><br /> <br /><p>Although we can see a difference between these two patches by eye, it’s hard to figure out what explicit features discriminate cell pixels from background pixels when using image patches. Even if we knew these features, it would be hard to figure out how to precisely and effectively combine them into rules to classify image patches. We therefore turn to statistical machine learning (ML) techniques. These powerful techniques perform extremely well in computer vision tasks. We’ll thus adapt them to our pixel-patch classification task. Specifically, given a set of representative pixel-patches from our task whose labels are known (a ‘training set’), these ML techniques will learn features and rules to effectively classify other pixel-patches coming from the same distribution.<p>We’re going to use a specific technique - ‘deep learning’ based convolutional neural networks (CNNs) - that’s currently very popular and successful. If you’re not familiar with deep learning or convolutional networks for image classification in general, the following <a href="http://cs231n.stanford.edu/">class</a> provides an excellent introduction.<p>These posts will walk us step by step through implementing these methods. We’ll implement examples using short, simple Python scripts in Google’s <a href="https://www.tensorflow.org/">Tensorflow</a> environment. In particular, these posts will:<ol><li><a href="https://levinejh.github.io/segmentation-first-demo">Demonstrate</a> training simple convolutional neural network to segment cells in microscopy images pixel-by-pixel.<li><a href="https://levinejh.github.io/segmentation-architectures">Explore</a> how simple variations in network architecture affect segmentation accuracy.<li><a href="https://levinejh.github.io/segmentation-fully-connected">Accelerate</a> a naive implementation almost 1000x by using published algorithms to make the network ‘fully convolutional.’</ol><p>A final note - if you’re looking for a great peer-reviewed research article on this subject, I can do no better than to point you to this <a href="http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005177">fantastic paper</a> by my friend <a href="http://www.bbe.caltech.edu/content/david-van-valen">David van Valen</a>. Please also check out his <a href="https://vanvalen.github.io/about/">DeepCell</a> software and give it a try!<footer><p>© 2017 Feather<p><a href="#top">[Back to Top]</a></footer></div>
