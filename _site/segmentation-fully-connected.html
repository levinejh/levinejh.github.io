<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1"><title>Segmentation 4 - Fast and Fully Connected</title><style> /*! normalize.css v7.0.0 | MIT License | github.com/necolas/normalize.css */html{line-height:1.15;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}article,aside,footer,header,nav,section{display:block}h1{font-size:2em;margin:.67em 0}figcaption,figure,main{display:block}figure{margin:1em 40px}hr{box-sizing:content-box;height:0;overflow:visible}pre{font-family:monospace, monospace;font-size:1em}a{background-color:transparent;-webkit-text-decoration-skip:objects}abbr[title]{border-bottom:none;text-decoration:underline;text-decoration:underline dotted}b,strong{font-weight:inherit}b,strong{font-weight:bolder}code,kbd,samp{font-family:monospace, monospace;font-size:1em}dfn{font-style:italic}mark{background-color:#ff0;color:#000}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}audio,video{display:inline-block}audio:not([controls]){display:none;height:0}img{border-style:none}svg:not(:root){overflow:hidden}button,input,optgroup,select,textarea{font-family:sans-serif;font-size:100%;line-height:1.15;margin:0}button,input{overflow:visible}button,select{text-transform:none}button,html [type='button'],[type='reset'],[type='submit']{-webkit-appearance:button}button::-moz-focus-inner,[type='button']::-moz-focus-inner,[type='reset']::-moz-focus-inner,[type='submit']::-moz-focus-inner{border-style:none;padding:0}button:-moz-focusring,[type='button']:-moz-focusring,[type='reset']:-moz-focusring,[type='submit']:-moz-focusring{outline:1px dotted ButtonText}fieldset{padding:.35em .75em .625em}legend{box-sizing:border-box;color:inherit;display:table;max-width:100%;padding:0;white-space:normal}progress{display:inline-block;vertical-align:baseline}textarea{overflow:auto}[type='checkbox'],[type='radio']{box-sizing:border-box;padding:0}[type='number']::-webkit-inner-spin-button,[type='number']::-webkit-outer-spin-button{height:auto}[type='search']{-webkit-appearance:textfield;outline-offset:-2px}[type='search']::-webkit-search-cancel-button,[type='search']::-webkit-search-decoration{-webkit-appearance:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}details,menu{display:block}summary{display:list-item}canvas{display:inline-block}template{display:none}[hidden]{display:none}.highlight{background:#282a36;color:#f8f8f2}.highlight .hll,.highlight .s,.highlight .sa,.highlight .sb,.highlight .sc,.highlight .dl,.highlight .sd,.highlight .s2,.highlight .se,.highlight .sh,.highlight .si,.highlight .sx,.highlight .sr,.highlight .s1,.highlight .ss{color:#f1fa8c}.highlight .go{color:#44475a}.highlight .err,.highlight .g,.highlight .l,.highlight .n,.highlight .x,.highlight .p,.highlight .ge,.highlight .gr,.highlight .gh,.highlight .gi,.highlight .gp,.highlight .gs,.highlight .gu,.highlight .gt,.highlight .ld,.highlight .no,.highlight .nd,.highlight .ni,.highlight .ne,.highlight .nn,.highlight .nx,.highlight .py,.highlight .w,.highlight .bp{color:#f8f8f2}.highlight .gh,.highlight .gi,.highlight .gu{font-weight:bold}.highlight .ge{text-decoration:underline}.highlight .bp{font-style:italic}.highlight .c,.highlight .ch,.highlight .cm,.highlight .cpf,.highlight .c1,.highlight .cs{color:#6272a4}.highlight .kd,.highlight .kt,.highlight .nb,.highlight .nl,.highlight .nv,.highlight .vc,.highlight .vg,.highlight .vi,.highlight .vm{color:#8be9fd}.highlight .kd,.highlight .nb,.highlight .nl,.highlight .nv,.highlight .vc,.highlight .vg,.highlight .vi,.highlight .vm{font-style:italic}.highlight .na,.highlight .nc,.highlight .nf,.highlight .fm{color:#50fa7b}.highlight .k,.highlight .o,.highlight .cp,.highlight .kc,.highlight .kn,.highlight .kp,.highlight .kr,.highlight .nt,.highlight .ow{color:#ff79c6}.highlight .m,.highlight .mb,.highlight .mf,.highlight .mh,.highlight .mi,.highlight .mo,.highlight .il{color:#bd93f9}.highlight .gd{color:#f55}*{margin:0;padding:0}*,*:before,*:after{box-sizing:inherit}html{box-sizing:border-box;font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{color:#282a36;font-family:-apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol';font-size:16px;line-height:1.5;word-wrap:break-word}a{color:#0366d6;font-weight:700;text-decoration:none}a:hover,a:active{text-decoration:underline}img{border-style:none;box-sizing:content-box;max-width:100%}h1,h2,h3,h4,h5,h6{font-weight:700;margin-top:40px;margin-bottom:10px;line-height:1.1}h1{font-size:2.25em}h2{font-size:1.5em}h3{font-size:1.25em}h4{font-size:1em}h5{font-size:.875em}h6{font-size:.75em}p,ul,ol{margin-bottom:20px}ul,ol{padding:0 0 0 40px;margin:0 0 20px 0}ol ol,ul ol{list-style-type:lower-roman}ul ul ol,ul ol ol,ol ul ol,ol ol ol{list-style-type:lower-alpha}li{margin-bottom:10px}input,select,textarea,button{font-family:inherit;font-size:inherit;line-height:inherit}pre{margin:40px -36vw;padding:40px 36vw;overflow:auto;font-size:1em;line-height:1.5;white-space:pre;word-wrap:normal;font-family:'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace}pre code{background-color:transparent;border-radius:0;font-size:100%;padding:0}code,tt{padding:5px;margin:0;font-size:85%;background-color:rgba(27,31,35,0.05);border-radius:3px;font-family:'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace}small{font-size:90%}strong{font-weight:700}.anchorjs-link,.anchorjs-link:hover{text-decoration:none !important}.visible-area{overflow:hidden}.container{margin:80px auto 40px;max-width:750px;padding:0 20px;width:100%}.introduction{margin:0;line-height:1.5}.list__item{margin-top:20px}.post__date{font-weight:700;margin:0 0 10px 0;text-transform:uppercase}.post__title{font-size:2.25em;margin:0 0 10px 0}@media only screen and (min-width: 768px){.post__title{font-size:3.75em}}.post__author{margin:0 0 40px 0}footer{color:#6a737d;display:flex;font-size:80%;justify-content:space-between;margin-top:40px}footer p{margin-bottom:0}.blue{color:#0366d6}.monospace{font-family:'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace}</style><body><div class="visible-area"><div class="container"><header><p class="post__date">27 September 2017<h1 class="post__title"><a name="top"></a>Segmentation 4 - Fast and Fully Connected</h1><p class="post__author"><em>by</em> <a href="https://levinejh.github.io/">Joe Levine</a></header><p>Our previous posts glossed over a crucial practical detail. How did we actually segment images? The careful reader will note I punted that discussion to a later post. So here it is.<p>Segmenting images with convolutional networks presents a bit of a conundrum. Recall the method’s main idea - classify each pixel using a small surrounding image patch to provide contextual information about its identity. This sounds great - until you think about classifying two adjacent pixels.<p>Two adjacent pixels share almost identical surrounding image patches. In our case, two adjacent 31x31 pixel patches overlap by almost 97%. Yet to classify both pixels, we ask our CNN to perform identical computations on both pixel-patches despite the overwhelming overlap, leading to loads of redundant computation.<p>This feels like a lot of wasted effort. There must be a better way.<p>And of course, there is. The solution people employ is the <em>fully convolutional network</em>. While a naive segmentation network classifies by processing individual pixel patches one at a time, a fully convolutional network processes the entire image in one shot without excessive redundant computation. It does this by arranging network’s filters in a clever way to reuse computed information. And in fact, one can convert a standard convolutional network for segmentation into a fully convolutional network.<p>Several research papers discuss the fully convolutional concept. I’ll discuss an implementation of a <a href="https://arxiv.org/abs/1412.4526">preprint</a> by Li <em>et al</em> 2014, “Highly Efficient Forward and Backward Propagation of Convolutional Neural Networks for Pixelwise Classification.” This preprint describes a very accessible method with which to convert a standard patch-wise convolutional segmentation network to a fully convolutional network. The method converts each operation layer of the standard network into its fully convolutional counterpart using a concept they call “d-regulary sparse kernels.” <br /><div style="text-align:center"> <img src="assets/2017-09-27-colonel.jpg" width="400" height="300" /><figcaption>We're talking about kernels, not colonels.</figcaption></div><p><br /> What do Li <em>et al</em>’s d-regularly sparse kernels entail?<p>The basic idea is to first transform each operation in the original network into a ‘sparse,’ or ‘dilated’ version. An operator is dilated by inserting zero rows and columns between its original values, as in this figure from the preprint:<div style="text-align:center"> <img src="assets/2017-09-27-dilation-matrix-diagram.png" width="600" height="500" /><figcaption>Dilating an operator by factor <i>d</i> inserts <i>d</i> zero rows and columns between the operator's original values. Top - original convolutional (a) and Max-Pool (b) kernels. Bottom - dilated kernels with dilation factor (c) 2 and (d) 3. (Source - Li *et al* 2014.)</figcaption></div><p><br /> <br /><p>Beginning at the network’s input, we initially do not dilate operators at all. Once, however, we hit an operator with stride greater than 1, we dilate all subsequent operations by that stride. Each subsequent operator with stride greater than one contributes multiplicatively to the dilation. For example, all operators following a stride 3 convolution and then a stride 2 max pool would be dilated by 6. We end up creating a fully convolutional network using these dilated operators in place of their corresponding original operators.<p>For those interested in a visual summary of these details, I suggest consulting Figure 3 of the original <a href="https://arxiv.org/abs/1412.4526">preprint</a>. The figure is a little challenging to parse on first reading, but once you see it you’ll grasp the mapping well.<p>We can readily implement this method using Tensorflow. Let’s write out how to do this for the convolutional portions of our original network. The key option to use is <code class="highlighter-rouge">dilation_rate</code> in <code class="highlighter-rouge">tf.nn.convolution()</code> and <code class="highlighter-rouge">tf.nn.pool()</code>. This allows us to directly implement the d-regularly sparse kernels of Li <em>et al</em>.<div class="highlighter-rouge"><pre class="highlight"><code>d = 1

h1_conv = tf.nn.convolution(x, W1, padding = 'VALID', strides = [1,1], dilation_rate = [d,d]) + b1
h1_relu = tf.nn.relu(h1_conv)
h1_pool = tf.nn.pool(h1_relu, window_shape = [2,2], pooling_type = 'MAX', padding = 'VALID', dilation_rate = [d,d], strides = [1,1])

d = d * 2 # h1_pool originally had stride of 2

h2_conv = tf.nn.convolution(h1_pool, W2, padding = 'VALID', strides = [1,1], dilation_rate = [d,d]) + b2
h2_relu = tf.nn.relu(h2_conv)
h3_conv = tf.nn.convolution(h2_relu, W3, padding = 'VALID', strides = [1,1], dilation_rate = [d,d]) + b3
h3_relu = tf.nn.relu(h3_conv)
h3_pool = tf.nn.pool(h3_relu, window_shape = [2,2], pooling_type = 'MAX', padding = 'VALID', dilation_rate = [d,d], strides = [1,1])

d = d * 2 # h3_pool originally had stride of 2

h4_conv = tf.nn.convolution(h3_pool, W4, padding = 'VALID', strides = [1,1], dilation_rate = [d,d]) + b4
h4_relu = tf.nn.relu(h4_conv)
h5_conv = tf.nn.convolution(h4_relu, W5, padding = 'VALID', strides = [1,1], dilation_rate = [d,d]) + b5
h5_relu = tf.nn.relu(h5_conv)
</code></pre></div><p>The rest of the code organization (loading data, setting up variables and placeholders, running a Tensorflow session) follows the structure described in the second post. It’s pretty standard, and I omit it here for brevity. One may need to do some fiddling with memory - on my system I needed to break the segmented image up into four pieces to fit the entire operation in memory.<p>How fast is this code? On an Amazon <code class="highlighter-rouge">g2.xlarge</code> instance this algorithm segments a 1040x1392 image in 3.5 seconds. The naive patch-by-patch approach takes 48 minutes.<p>That’s an 822x speed-up.<p>In other words, a game changer.<footer><p>© 2017 Feather<p><a href="#top">[Back to Top]</a></footer></div>
